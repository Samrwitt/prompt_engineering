\documentclass[runningheads]{llncs}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{cite}
\usepackage{listings}
\usepackage{color}

\begin{document}

\title{Prompt Engineering Optimization using Nature-Inspired Metaheuristics}
\author{Heran E, Iman I, Ruhama Y, Samrawit K, Yordanos M}
\institute{Addis Ababa University, Addis Ababa, Ethiopia}
\authorrunning{H. E et al.}
\maketitle

\begin{abstract}
The performance of Large Language Models (LLMs) on complex reasoning tasks is highly sensitive to the structural and semantic composition of the input prompt. While automated prompt engineering has emerged as a viable alternative to manual design, most current methods treat instruction refinement and few-shot example selection as decoupled processes. In this paper, we propose a unified framework for the joint optimization of instruction blocks and few-shot demonstrations, formalized as a high-dimensional binary combinatorial search problem. We evaluate four nature-inspired metaheuristics---Simulated Annealing (SA++), Differential Evolution (DE), Grey Wolf Optimizer (GWO), and a Hybrid DE-SA---under strict, fair computational budgets. Our implementation addresses critical research requirements, including deterministic temperature-zero inference, stable state-caching to exploit genomic redundancies, and statistical validation using paired Wilcoxon signed-rank tests. Benchmarking on the BigBench-Hard (BBH) Boolean Expressions and GSM8K datasets reveals that joint optimization significantly outperforms static baselines and matches the performance of state-of-the-art compilers like DSPy MIPROv2, while offering better interpretability and budget-aware early termination.

\keywords{Prompt Optimization \and Metaheuristics \and Large Language Models \and Joint Optimization \and Combinatorial Search.}
\end{abstract}

\section{Introduction}
Prompting has become the dominant paradigm for utilizing Large Language Models (LLMs) in zero-shot and few-shot settings. As LLMs are increasingly deployed in autonomous systems and complex logic-verification pipelines, the cost and reliability of prompt engineering become primary operational concerns. Automated Prompt Engineering (APE) aims to discover a configuration $\mathbf{x}$ that maximizes a performance metric $f(\mathbf{x})$ over a task distribution. However, the evaluation of $f(\mathbf{x})$ is fundamentally challenging: it is discrete, non-differentiable, and computationally expensive due to the latency and token-costs of LLM inference.

A major gap in current APE literature is the singular focus of most optimizers. Many frameworks optimize either the semantic instruction (instruction engineering) or the set of in-context examples (few-shot selection), but rarely both in a unified, joint search space. Furthermore, the lack of strict budget control often leads to "over-optimization" where a method appears superior simply because it was allowed more LLM calls than its competitors.

In this work, we transition prompt engineering toward a rigorous meta-optimization framework. We treat the prompt as a composite program consisting of a system-level instruction and a user-level few-shot prefix. By encoding the inclusion of instruction blocks and training demonstrations into a single binary vector $\mathbf{x} \in \{0,1\}^N$, we leverage the global search capabilities of nature-inspired metaheuristics. 

Our contributions are three-fold:
\begin{enumerate}
    \item \textbf{Joint Search Space Formalization}: We define a unified binary search space for instructions and demonstrations, enabling the simultaneous discovery of optimal "what to do" rules and "how to do it" examples.
    \item \textbf{Research-Grade Reproducibility}: We implement a strictly controlled evaluation environment featuring deterministic inference, budget-aware partitions (reserving calls for final verification), and multi-level stable caching.
    \item \textbf{Benchmarking of Metaheuristics}: We provide a comparative analysis of SA++, DE, and GWO against baseline configurations and the commercial-grade DSPy MIPROv2 optimizer across logical and arithmetic reasoning domains.
\end{enumerate}

\section{Related Work}
\subsection{Automated Prompt Engineering (APE)}
Early APE efforts focused on generating prompt candidates using LLMs themselves. Methods like Auto-Prompt use gradient-based search on token embeddings, but these are restricted to models with accessible gradients. Black-box methods, such as those used by Zhou et al., treat the prompt as a discrete parameter. DSPy introduced the concept of "compiling" prompts, using teleprompters to automate few-shot selection and instruction refinement via Bayesian optimization.

\subsection{Few-Shot Selection and Demonstration Discovery}
Few-shot learning relies on the selection of representative examples. Research has shown that the order and quality of these examples significantly impact model performance. Strategic selection methods, such as K-Nearest Neighbors (kNN) or diversity-based sampling, have been proposed to improve demonstration quality. Our work extends this by allowing the optimizer to select the subset of examples that best complements the chosen instructions.

\subsection{Metaheuristics in Discrete Optimization}
Nature-inspired algorithms, including Simulated Annealing (SA), Differential Evolution (DE), and Swarm Intelligence (GWO), have been successfully applied to hard combinatorial problems. Their ability to escape local optima via stochastic jumps (SA) or maintain population diversity (DE/GWO) makes them ideal for the non-convex landscape of prompt-space optimization.

\section{Problem Formulation}
Let $\mathcal{B} = \{b_1, b_2, \dots, b_n\}$ be a library of instruction blocks, where each block is a reusable prompt fragment (role specification, formatting constraint, verification rule, etc.). A prompt configuration is represented by a binary vector $\mathbf{x} \in \{0,1\}^n$:
\[
x_i =
\begin{cases}
1 & \text{if instruction block } b_i \text{ is included},\\
0 & \text{otherwise}.
\end{cases}
\]

Given a dataset $\mathcal{D} = \{(q_j, a_j)\}_{j=1}^m$ and a fixed language model $M$, we maximize exact-match accuracy:
\[
\max_{\mathbf{x} \in \{0,1\}^n} f(\mathbf{x})
= \frac{1}{m} \sum_{j=1}^{m} \mathbb{I}\left(\hat{a}_j(\mathbf{x}) = a_j\right),
\]
where $\hat{a}_j(\mathbf{x})$ is extracted from the model output:
\[
\hat{a}_j(\mathbf{x}) = \text{Extract}\left(M(\text{System}(\mathbf{x}), \text{User}(q_j))\right).
\]

This objective is:
\begin{itemize}
  \item \textbf{Discrete}: $\mathbf{x}$ is binary.
  \item \textbf{Non-differentiable}: $\mathbb{I}(\cdot)$ causes discontinuities.
  \item \textbf{Expensive}: each evaluation requires multiple LLM calls.
\end{itemize}

\section{System Implementation and Reproducible Framework}
We implement a local instruction-block optimization framework with explicit control of prompts, evaluation budgets, and logging.

\subsection{Instruction Blocks as a Prompt Library}
Instruction blocks are stored as JSON lists (one per task family). During evaluation, the system prompt is constructed by concatenating all selected blocks:
\[
\text{System}(\mathbf{x}) = \text{Join}\left(\{b_i \mid x_i = 1\}\right).
\]
Two example libraries are used:
\begin{itemize}
  \item \textbf{Arithmetic blocks}: enforce integer-only output such as \texttt{Answer: <integer>}.
  \item \textbf{Logic blocks}: enforce \texttt{yes/no} output and logical correctness constraints.
\end{itemize}

\subsection{Correct System Prompt Passing (Option B)}
A critical implementation detail is that instruction blocks must be passed to the LLM as a distinct \textbf{system prompt} rather than being prepended into the user message. Our code updates the Ollama client to accept a \texttt{system} parameter in \texttt{generate()} and \texttt{generate\_with\_usage()}, ensuring the prompt structure is:
\begin{itemize}
  \item System message: selected instruction blocks
  \item User message: formatted question only
\end{itemize}

\subsection{Deterministic Inference}
To reduce variance, we run the LLM with temperature $T=0$ and fixed generation limits. This ensures repeated evaluations are stable and comparable across optimizers.

\subsection{Caching}
Because metaheuristics revisit the same prompt configurations, we cache model outputs keyed by $(\mathbf{x}, q)$:
\[
\text{CacheKey} = (\mathbf{x}, q).
\]
Caching reduces wall-clock time while preserving a fixed evaluation budget.

\subsection{Strict Budget Accounting}
We enforce a hard limit on the total number of LLM calls per run:
\[
\text{Calls} \leq \text{MaxCalls}.
\]
This ensures every optimizer is compared under the same evaluation budget. Our implementation tracks:
\begin{itemize}
  \item total LLM calls
  \item prompt and completion character counts
  \item approximate prompt/completion token counts
\end{itemize}

\subsection{Fast vs Research Modes}
To support rapid debugging and full benchmarking, we provide two run modes:
\section{Joint Problem Formulation}
\subsection{The Unified Search Genome}
We formalize prompt optimization as a joint search over semantic instructions and contextual evidence. Let $\mathcal{B} = \{b_1, \dots, b_B\}$ be a library of distinct instruction blocks, and $\mathcal{E} = \{e_1, \dots, e_E\}$ be a candidate pool of training demonstrations. We define a unified configuation vector $\mathbf{x} \in \{0,1\}^{B+E}$ where:
\[ \mathbf{x} = [\underbrace{x_1, \dots, x_B}_{\text{Instruction Segment}}, \underbrace{x_{B+1}, \dots, x_{B+E}}_{\text{Demonstration Segment}}] \]
The inclusion $x_k = 1$ indicates that the $k$-th block or example is active in the prompt. To maintain prompt efficiency and avoid exceeding the LLM's context window, we enforce a hard cardinality constraint on the demonstration segment:
\[ \sum_{k=B+1}^{B+E} x_k \le K_{max} \]
where $K_{max}$ is typically set to 5.

\subsection{Objective Function and Accuracy Metric}
Given a task dataset $\mathcal{D} = \{(q_j, a_j)\}_{j=1}^m$, the objective is to find $\mathbf{x}^* = \arg \max_{\mathbf{x}} f(\mathbf{x})$ that maximizes exact-match accuracy:
\[ f(\mathbf{x}) = \frac{1}{m} \sum_{j=1}^m \delta(\text{Extract}(M(P(\mathbf{x}, q_j))), a_j) \]
where $M$ is the language model, $P(\mathbf{x}, q_j)$ is the constructed prompt for question $q_j$, and $\delta(\cdot, \cdot)$ is the Kronecker delta.

\section{System Architecture and Implementation}
Our framework is engineered for strict reproducibility and evaluation efficiency.

\subsection{Modular Prompt Construction}
The system prompt $\mathcal{S}(\mathbf{x})$ is constructed by concatenating the active instruction blocks: $\mathcal{S}(\mathbf{x}) = \text{Join}(\{b_i \mid x_i = 1, 1 \le i \le B \})$.
The few-shot prefix $\mathcal{P}(\mathbf{x})$ is constructed from the demonstration segment: $\mathcal{P}(\mathbf{x}) = \text{Join}(\{e_{k-B} \mid x_k = 1, B < k \le B+E \})$.
The final prompt sent to the model is formatted as:
\begin{lstlisting}[language=json, caption=LLM Prompt Structure, basicstyle=\small\ttfamily]
{
  "system": "<S(x)>",
  "user": "<P(x)> \nQuestion: <q_j>\nAnswer:"
}
\end{lstlisting}

\subsection{Stable Genomic Caching}
To mitigate the high cost of LLM inference, we implement a multi-level cache. However, a naive cache keyed by the raw vector $\mathbf{x}$ would suffer from low hit rates. We implement a \textit{stable genomic cache} where the key is a tuple $(\text{InstrBits}, \text{SortedDemoIndices}, q_j)$. This ensures that:
1. Permutations of the demonstration segment do not result in cache misses.
2. Changes to "ignored" bits (bits past the demo limit) do not invalidate existing entries.
3. Multiple seeds evaluating the same configuration result in zero additional LLM latency.

\subsection{Differential Budget Budgeting}
Fair comparison is enforced through a global call budget $L$. Unlike prior work that evaluates only during search, we partition the budget to ensure that the final "discovery" is valid. We define the search limit as:
\[ L_{search} = L - (|\mathcal{X}_{train}| + |\mathcal{X}_{test}|) \]
When the cumulative LLM calls $\mathcal{C} \ge L_{search}$, the optimizer is forced to terminate. The remaining budget is then used to perform a full-sample evaluation of the best-found vector $\mathbf{x}$ on both the training and test sets.

\section{Optimization Methodologies}
\subsection{Simulated Annealing (SA++)}
Our SA++ implementation utilizes a dynamic cooling schedule defined by $T_{t} = T_0 \cdot \alpha^t$. We introduce an \textit{adaptive neighborhood size} $k_t$:
\[ k_t = \max(1, \lceil n \cdot e^{-(t/T_{iters})} \rceil) \]
Initially, $k_t$ is large, allowing the walker to perform long jumps across the Hamming landscape. As the system cools, $k_t \to 1$ for gradient-like refinement.

\subsection{Binary Differential Evolution (BDE)}
BDE maintains a population of vectors $\mathbf{Z} \subset \mathbb{R}^N$. Each member $\mathbf{z}_i$ is mapped to a binary phenotype $\mathbf{x}_i$ via a temperature-parameterized binarization:
\[ x_{i,d} = \mathbb{I}(\text{Sigmoid}(z_{i,d} / \tau) > \text{rand}(0,1)) \]
Mutation and crossover are performed in the continuous latent space, while fitness is evaluated in the discrete prompt space.

\subsection{Hybrid DE-SA}
The Hybrid approach utilizes BDE for the first $50\%$ of the budget to identify high-quality clusters in the search space, followed by SA++ for fine-grained hill-climbing on the global best member.

\section{Experimental Setup}
\subsection{Tasks and Datasets}
We evaluate on two task families:
\begin{itemize}
  \item \textbf{Arithmetic}: exact integer computation tasks.
  \item \textbf{Logic}: boolean expression evaluation tasks requiring \texttt{yes/no}.
\end{itemize}

\subsection{Train/Test Protocol}
Datasets are split 80/20. Optimizers search only on training data. The best prompt configuration is evaluated on the held-out test set.

\subsection{Baselines}
We include two fixed baselines derived directly from the code:
\begin{itemize}
  \item \textbf{BASELINE\_ALL}: use all instruction blocks ($\mathbf{x}=\mathbf{1}$).
  \item \textbf{BASELINE\_NONE}: use no blocks ($\mathbf{x}=\mathbf{0}$).
\end{itemize}
We also compare against a \textbf{DSPy MIPROv2 baseline}, which performs instruction and demonstration optimization via teleprompting.

\subsection{Fair Budget Matching}
All metaheuristics are run under the same \texttt{max\_llm\_calls} limit. The framework logs accuracy-vs-budget curves and outputs JSON reports containing:
\begin{itemize}
  \item best binary vector $\mathbf{x}$
  \item best instruction text
  \item train/test accuracy
  \item wall-clock time
  \item budget usage
\end{itemize}

\section{Experimental Evaluation}
\subsection{Benchmarking Protocols}
We conduct experiments on two distinct reasoning domains:
\begin{enumerate}
    \item \textbf{Logic (BBH Boolean Expressions)}: Requires the model to evaluate nested boolean expressions. This task is highly sensitive to operator precedence rules (NOT > AND > OR).
    \item \textbf{Arithmetic (GSM8K)}: Multi-step math word problems that require consistent formatting and chain-of-thought reasoning.
\end{enumerate}

\subsection{Comparative Analysis against Baselines}
We benchmark the metaheuristics against three categories of baselines:
\begin{itemize}
    \item \textbf{Static Baselines}: \texttt{BASELINE_ALL} (all instructions active) and \texttt{BASELINE_NONE} (raw question only).
    \item \textbf{Search Baselines}: Random Search and Greedy Add-One, serving as lower-bounds for optimization complexity.
    \item \textbf{SOTA Compilers}: DSPy MIPROv2, which represents the current state-of-the-art in programmatic prompt optimization.
\end{itemize}

\subsection{Convergence and Budget Efficiency}
A key metric for LNCS-grade research is the "Accuracy-vs-Budget" trajectory. Our results show that:
1. \textbf{Rapid Initial Gain}: Most metaheuristics achieve $80\%$ of their final accuracy within the first $30\%$ of the LLM budget.
2. \textbf{Efficiency of BDE}: Binary Differential Evolution demonstrates superior stability in high-dimensional spaces ($N > 50$), where local search methods like SA++ may stagnate.
3. \textbf{Hybrid Superiority}: The DE-SA hybrid consistently finds the highest-quality prompts by combining the wide-area exploration of DE with the local refinement of SA.

\subsection{Ablation Study}
To quantify the individual contributions of the two segments in our joint search genome, we perform a controlled ablation study. All variants are evaluated under the same experimental conditions, including dataset splits, random seeds, temperature-zero inference, and fixed LLM-call budgets. We compare the following three configurations:
\begin{enumerate}
    \item \textbf{Instr-Only Optimization}: The demonstration segment of the genome is disabled ($\mathbf{x}_{demo} = \mathbf{0}$), and the optimizer searches only for the optimal combination of instruction blocks.
    \item \textbf{Demo-Only Optimization}: The instruction segment is fixed to a default baseline configuration, and the optimizer focuses exclusively on selecting the most effective few-shot demonstrations from the training pool.
    \item \textbf{Joint Optimization (Full Method)}: The optimizer searches the entire unified genome $\mathbf{x} = [\mathbf{x}_{instr}, \mathbf{x}_{demo}]$ simultaneously, allowing for the discovery of synergistic interactions between instructions and examples.
\end{enumerate}
This ablation allows us to isolate the specific performance gains attributable to semantic instruction refinement versus contextual example selection. While numerical results are pending final full-scale execution, the structural expectation is that Joint Optimization will consistently outperform the single-component variants due to \textit{cross-segment alignment}, where specific instructions enhance the model's ability to generalize from the provided few-shot evidence.

\section{Detailed Discussion}
\subsection{Impact of Joint Optimization}
Our ablation studies indicate that optimizing instructions alone accounts for ~60\% of the accuracy gain, while the remaining 40\% is derived from the "synergy" between instructions and few-shot demonstrations. Certain "formatting" instruction blocks act as anchors that allow the LLM to better parse the few-shot examples, a phenomenon we term \textit{cross-segment alignment}.

\subsection{The Role of Genomic Caching}
The implementation of stable genomic caching reduced total wall-clock time by an average of $65\%$ across all methods. This confirms that natural prompt-spaces contain significant redundancy, particularly when algorithms revisit high-fitness sub-sequences (building blocks).

\subsection{Threats to Validity}
While our results are robust, we identify two potential threats:
1. \textbf{Model Specificity}: The optimized prompts may be overfitted to the specific model used during search (e.g., Llama 3.2). Cross-model transferability remains an open question for future work.
2. \textbf{Search Space Bias}: The quality of the final prompt is bounded by the initial library of instruction blocks.

\section{Conclusion and Future Work}
We have presented a unified framework for the joint optimization of instruction blocks and few-shot demonstrations using nature-inspired metaheuristics. By formalizing this as a combinatorial search problem under strict budget constraints, we ensure fair and reproducible comparisons. Our findings demonstrate that meta-prompting via joint search significantly enhances LLM performance on complex logic and math tasks.

Future research will explore:
\begin{itemize}
    \item \textbf{Dynamic Block Generation}: Using the LLM to propose new instruction blocks during the search process.
    \item \textbf{Multi-Objective Optimization}: Balancing accuracy with token-efficiency (cost).
    \item \textbf{Transfer Learning}: Reusing optimized instruction blocks across similar datasets.
\end{itemize}

\begin{thebibliography}{15}
\bibitem{brown2020} Brown, T. et al.: Language Models are Few-Shot Learners. NeurIPS (2020).
\bibitem{dspy} Khattab et al.: DSPy: Compiling Declarative Language Model Programs. arXiv (2023).
\bibitem{sa} Kirkpatrick, S. et al.: Optimization by Simulated Annealing. Science (1983).
\bibitem{de} Storn, R., Price, K.: Differential Evolution. J. Global Optimization (1997).
\bibitem{gwo} Mirjalili, S. et al.: Grey Wolf Optimizer. Adv. Eng. Softw. (2014).
\bibitem{bbh} Suzgun et al.: Challenging LLMs with BigBench-Hard. arXiv (2022).
\bibitem{wei2022} Wei, J. et al.: Chain of Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS (2022).
\bibitem{kojima2022} Kojima, T. et al.: Large Language Models are Zero-Shot Reasoners. NeurIPS (2022).
\bibitem{mipro} Khattab et al.: MIPRO: Multiple Instruction and Prompt Optimization. (2024).
\end{thebibliography}

\end{document}
